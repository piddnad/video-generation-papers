# Video Generation Papers

A list of recent papers on video generation.

## Video Generation

2023.04

- (2023.04.12) [arXiv; Google] **DreamPose**: Fashion Image-to-Video Synthesis via Stable Diffusion [[page](https://grail.cs.washington.edu/projects/dreampose/), [paper](https://arxiv.org/abs/2304.06025), [code](https://github.com/johannakarras/DreamPose)]
- (2023.04.03) [arXiv; Tencent] **Follow Your Pose**: Pose-Guided Text-to-Video Generation using Pose-Free Videos [[page](https://follow-your-pose.github.io/), [paper](https://arxiv.org/abs/2304.01186), [code](https://github.com/mayuelala/FollowYourPose)]

2023.03

- (2023.03.23) [arXiv; Picsart AI Resarch] **Text2Video-Zero**: Text-to-Image Diffusion Models are Zero-Shot Video Generators [[page](https://text2video-zero.github.io/), [paper](https://arxiv.org/abs/2303.13439), [code](https://github.com/Picsart-AI-Research/Text2Video-Zero)]
- (2023.03.22) [arXiv; Microsoft] **NUWA-XL**: Diffusion over Diffusion for eXtremely Long Video Generation [[page](https://msra-nuwa.azurewebsites.net/), [paper](https://arxiv.org/abs/2303.12346)]
- (2023.03.20) [arXiv; KAIST] Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers [[page](https://sites.google.com/view/mebt-cvpr2023/home), [paper](https://arxiv.org/abs/2303.11251), [code](https://github.com/Ugness/MeBT)]
- (2023.03.15) [CVPR 2023; Alibaba] **VideoFusion**: Decomposed Diffusion Models for High-Quality Video Generation [[paper](https://arxiv.org/abs/2303.08320), [code](https://github.com/modelscope/modelscope/tree/master/modelscope/models/multi_modal/video_synthesis)]

2023.02

- (2023.02.15) [CVPR 2023; Google] Video Probabilistic Diffusion Models in Projected Latent Space [[page](https://sihyun.me/PVDM/), [paper](https://arxiv.org/abs/2302.07685), [code](https://github.com/sihyun-yu/PVDM)]
- (2023.02.06) [arXiv; Runway] **(GEN1)** Structure and Content-Guided Video Synthesis with Diffusion Models [[page](https://research.runwayml.com/gen1), [paper](https://arxiv.org/abs/2302.03011)]

2022

- (2022.12.22) [arXiv; Tencent] **Tune-A-Video**: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation [[page](https://tuneavideo.github.io/), [paper](https://arxiv.org/abs/2212.11565), [code](https://github.com/showlab/Tune-A-Video)]
- (2022.12.10) [CVPR 2023; Google] MAGVIT: Masked Generative Video Transformer [[page](https://arxiv.org/abs/2212.05199), [paper](https://arxiv.org/abs/2212.05199), [code(coming soon)](https://github.com/MAGVIT/magvit)]
- (2022.11.23) [arXiv; Tencent] Latent Video Diffusion Models for High-Fidelity Long Video Generation [[page](https://yingqinghe.github.io/LVDM/), [paper](https://arxiv.org/abs/2211.13221), [code(coming soon)](https://github.com/YingqingHe/LVDM)]
- (2022.11.20) [arXiv; ByteDance] **MagicVideo**: Efficient Video Generation With Latent Diffusion Models [[page](https://magicvideo.github.io/), [paper](https://arxiv.org/abs/2211.11018)]
- (2022.10.05) [arXiv; Google] **Imagen Video**: High Definition Video Generation with Diffusion Models [[page](https://imagen.research.google/video/), [paper](https://arxiv.org/abs/2210.02303)]
- (2022.09.29) [ICLR 2023; Meta] **Make-A-Video**: Text-to-Video Generation without Text-Video Data [[page](https://makeavideo.studio/), [paper](https://arxiv.org/abs/2209.14792), [code](https://github.com/lucidrains/make-a-video-pytorch)]
- (2022.05.29) [ICLR 2023; THU] **CogVideo**: Large-scale Pretraining for Text-to-Video Generation via Transformers Paper [[demo](https://models.aminer.cn/cogvideo/), [paper](https://arxiv.org/abs/2205.15868), [code](https://github.com/THUDM/CogVideo)]
- (2022.04.07) [NIPS 2022; Google] Video Diffusion Models [[page](https://video-diffusion.github.io/), [paper](https://arxiv.org/abs/2204.03458), [code](https://github.com/lucidrains/video-diffusion-pytorch)]

## Video Editing

- (2023.03.16) [arXiv; Tencent] **FateZero** : Fusing Attentions for Zero-shot Text-based Video Editing [[page](https://fate-zero-edit.github.io/), [paper](https://arxiv.org/abs/2303.09535), [code](https://github.com/ChenyangQiQi/FateZero)]
- (2023.03.08) [arXiv; CUHK] **Video-P2P**: Video Editing with Cross-attention Control [[page](https://video-p2p.github.io/), [paper](https://arxiv.org/abs/2303.04761), [code](https://github.com/ShaoTengLiu/Video-P2P)]
- (2023.02.02) [arXiv; Google] **Dreamix**: Video Diffusion Models are General Video Editors [[page](https://dreamix-video-editing.github.io/), [paper](https://arxiv.org/abs/2302.01329)]
- (2023.01.30) [arXiv; UMD] Shape-aware Text-driven Layered Video Editing [[page](https://text-video-edit.github.io/), [paper](https://arxiv.org/abs/2301.13173)]
- (2022.04.05) [ECCV 2022; NVIDIA] **Text2LIVE**: Text-Driven Layered Image and Video Editing [[page](https://text2live.github.io/), [paper](https://arxiv.org/abs/2204.02491), [code](https://github.com/omerbt/Text2LIVE)]



